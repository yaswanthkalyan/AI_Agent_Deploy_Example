ğŸ¤– LangGraph AI Agent with Docker
This project is a simple yet powerful AI Agent app using LangGraph + FastAPI for backend logic and Streamlit for the frontend interface. It integrates Groq LLMs and Tavily Search to enhance response capabilities.

ğŸ“ Project Structure
bash
Copy
Edit
LangGraph-AI-Agent/
â”œâ”€â”€ .env                 # Environment variables (API keys)
â”œâ”€â”€ Dockerfile           # Docker setup (optional containerization)
â”œâ”€â”€ README.md            # Project documentation
â”œâ”€â”€ app.py               # FastAPI backend server
â”œâ”€â”€ requirements.txt     # All dependencies
â””â”€â”€ ui.py                # Streamlit frontend interface
âš™ï¸ Features
ğŸŒ LangGraph-powered AI agent

ğŸ§  Supports Groq-hosted LLMs like llama3-70b-8192, mixtral-8x7b-32768

ğŸ” Integrates Tavily for real-time web search

ğŸ“¡ Backend served via FastAPI

ğŸ’¬ User-friendly frontend via Streamlit

ğŸ§ª Requirements
Install all required Python dependencies:

bash
Copy
Edit
pip install -r requirements.txt
Make sure Python 3.9+ is installed.

ğŸ” Environment Setup
Create a .env file in the root directory:

ini
Copy
Edit
GROQ_API_KEY=your_groq_api_key_here
TAVILY_API_KEY=your_tavily_api_key_here
ğŸš€ Running the App
Step 1: Start FastAPI Backend
bash
Copy
Edit
python app.py
It will start on: http://127.0.0.1:8000/chat

Step 2: Launch Streamlit UI
In another terminal:

bash
Copy
Edit
streamlit run ui.py
It will open the app in your browser: http://localhost:8501

ğŸ“¦ Example Payload (API)
POST /chat

json
Copy
Edit
{
  "system_prompt": "You are a helpful assistant.",
  "model_name": "llama3-70b-8192",
  "messages": ["What's the capital of France?"]
}
ğŸ³ Docker (Optional)
To build and run with Docker:

bash
Copy
Edit
docker build -t langgraph-agent .
docker run -p 8000:8000 langgraph-agent
ğŸ§  Use Cases
Create smart assistant agents

Integrate live search with AI

Switch between different LLMs easily

ğŸ‘¨â€ğŸ’» Author
Yaswanth Kalyan Ponugoti
ML Engineer | Data Scientist | LangGraph Enthusiast
